% Generated by IEEEtranS.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{1}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtranS.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{Chollet16a}
\BIBentryALTinterwordspacing
F.~Chollet, ``Xception: Deep learning with depthwise separable convolutions,''
  \emph{CoRR}, vol. abs/1610.02357, 2016. [Online]. Available:
  \url{http://arxiv.org/abs/1610.02357}
\BIBentrySTDinterwordspacing

\bibitem{url2}
\BIBentryALTinterwordspacing
L.~et. al. (2017) Author discussion. [Online]. Available:
  \url{https://github.com/tensorflow/tensor2tensor/issues/9}
\BIBentrySTDinterwordspacing

\bibitem{url1}
\BIBentryALTinterwordspacing
------. (2017) {Github Code} github code. [Online]. Available:
  \url{https://github.com/tensorflow/tensor2tensor/blob/f9c859a7639831c6da864e360793a285613dcc5c/tensor2tensor/models/multimodel.py}
\BIBentrySTDinterwordspacing

\bibitem{KaiserGSVPJU17}
\BIBentryALTinterwordspacing
L.~Kaiser, A.~N. Gomez, N.~Shazeer, A.~Vaswani, N.~Parmar, L.~Jones, and
  J.~Uszkoreit, ``One model to learn them all,'' \emph{CoRR}, vol.
  abs/1706.05137, 2017. [Online]. Available:
  \url{http://arxiv.org/abs/1706.05137}
\BIBentrySTDinterwordspacing

\bibitem{ShazeerMMDLHD17}
\BIBentryALTinterwordspacing
N.~Shazeer, A.~Mirhoseini, K.~Maziarz, A.~Davis, Q.~V. Le, G.~E. Hinton, and
  J.~Dean, ``Outrageously large neural networks: The sparsely-gated
  mixture-of-experts layer,'' \emph{CoRR}, vol. abs/1701.06538, 2017. [Online].
  Available: \url{http://arxiv.org/abs/1701.06538}
\BIBentrySTDinterwordspacing

\end{thebibliography}
