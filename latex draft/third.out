\BOOKMARK [1][-]{section.1}{Introduction}{}% 1
\BOOKMARK [2][-]{subsection.1.1}{Usage of streaming data}{section.1}% 2
\BOOKMARK [2][-]{subsection.1.2}{Usage of anomaly detection}{section.1}% 3
\BOOKMARK [2][-]{subsection.1.3}{Usage of anomaly detection in streaming data}{section.1}% 4
\BOOKMARK [2][-]{subsection.1.4}{motivation}{section.1}% 5
\BOOKMARK [2][-]{subsection.1.5}{Constraints}{section.1}% 6
\BOOKMARK [2][-]{subsection.1.6}{General framework of anomaly detectors and summary of each step}{section.1}% 7
\BOOKMARK [3][-]{subsubsection.1.6.1}{Our Contribution}{subsection.1.6}% 8
\BOOKMARK [3][-]{subsubsection.1.6.2}{Details of sections coming up-next}{subsection.1.6}% 9
\BOOKMARK [1][-]{section.2}{Related Work and State of the art}{}% 10
\BOOKMARK [2][-]{subsection.2.1}{Unsupervised Anomaly Detection}{section.2}% 11
\BOOKMARK [2][-]{subsection.2.2}{Time series Introduction}{section.2}% 12
\BOOKMARK [3][-]{subsubsection.2.2.1}{Time series trend, seasonality and stationary vs non-stationary or concept drift}{subsection.2.2}% 13
\BOOKMARK [2][-]{subsection.2.3}{Time series based deep learning methods}{section.2}% 14
\BOOKMARK [3][-]{subsubsection.2.3.1}{fully connected methods}{subsection.2.3}% 15
\BOOKMARK [3][-]{subsubsection.2.3.2}{CNN based methods}{subsection.2.3}% 16
\BOOKMARK [3][-]{subsubsection.2.3.3}{LSTM based methods}{subsection.2.3}% 17
\BOOKMARK [2][-]{subsection.2.4}{Anomaly detection in time series}{section.2}% 18
\BOOKMARK [2][-]{subsection.2.5}{Unsupervised time series Anomaly detection}{section.2}% 19
\BOOKMARK [1][-]{section.3}{Proposed method}{}% 20
\BOOKMARK [2][-]{subsection.3.1}{General anomaly score based architecture and detailed explanation}{section.3}% 21
\BOOKMARK [2][-]{subsection.3.2}{Autoencoder based architecture}{section.3}% 22
\BOOKMARK [2][-]{subsection.3.3}{Multistep-Ahead prediction based architecture}{section.3}% 23
\BOOKMARK [2][-]{subsection.3.4}{Fully connected multistep ahead prediction architecture}{section.3}% 24
\BOOKMARK [2][-]{subsection.3.5}{Convolutional multistep ahead prediction architecture}{section.3}% 25
\BOOKMARK [2][-]{subsection.3.6}{LSTM multistep ahead prediction architecture}{section.3}% 26
\BOOKMARK [2][-]{subsection.3.7}{Autoencoder architecture}{section.3}% 27
\BOOKMARK [2][-]{subsection.3.8}{Fully Connected Autoencoder architecture}{section.3}% 28
\BOOKMARK [2][-]{subsection.3.9}{Convolutional Autoencoder architecture}{section.3}% 29
\BOOKMARK [2][-]{subsection.3.10}{LSTM Autoencoder architecture}{section.3}% 30
\BOOKMARK [2][-]{subsection.3.11}{Recency concept}{section.3}% 31
\BOOKMARK [2][-]{subsection.3.12}{Post-processing}{section.3}% 32
\BOOKMARK [2][-]{subsection.3.13}{Thresholding}{section.3}% 33
\BOOKMARK [2][-]{subsection.3.14}{Parameterinzing the anomaly score}{section.3}% 34
\BOOKMARK [2][-]{subsection.3.15}{Scoring}{section.3}% 35
\BOOKMARK [1][-]{section.4}{Empirical Formulation and Experiments}{}% 36
\BOOKMARK [2][-]{subsection.4.1}{Dataset}{section.4}% 37
\BOOKMARK [1][-]{section.5}{Results}{}% 38
\BOOKMARK [2][-]{subsection.5.1}{Comparing with others}{section.5}% 39
\BOOKMARK [1][-]{section.6}{Conclusion and Discussion}{}% 40
\BOOKMARK [1][-]{section.7}{Experiment Infrastructure}{}% 41
\BOOKMARK [2][-]{subsection.7.1}{Experiment Management using MLflow}{section.7}% 42
\BOOKMARK [2][-]{subsection.7.2}{Parallel execution using Docker}{section.7}% 43
\BOOKMARK [1][-]{section.8}{Best practices}{}% 44
\BOOKMARK [2][-]{subsection.8.1}{Moving from jupyterlab to pycharm}{section.8}% 45
\BOOKMARK [1][-]{section.9}{Reference Usage}{}% 46
\BOOKMARK [1][-]{section.10}{References}{}% 47
