\BOOKMARK [1][-]{section.1}{Introduction}{}% 1
\BOOKMARK [2][-]{subsection.1.1}{Usage of streaming data}{section.1}% 2
\BOOKMARK [2][-]{subsection.1.2}{Usage of anomaly detection in streaming data}{section.1}% 3
\BOOKMARK [2][-]{subsection.1.3}{Constraints}{section.1}% 4
\BOOKMARK [3][-]{subsubsection.1.3.1}{Access to only current data}{subsection.1.3}% 5
\BOOKMARK [3][-]{subsubsection.1.3.2}{Infinite lenght hence continuous training}{subsection.1.3}% 6
\BOOKMARK [3][-]{subsubsection.1.3.3}{Definitions related to time series}{subsection.1.3}% 7
\BOOKMARK [3][-]{subsubsection.1.3.4}{Anomaly Detection and types}{subsection.1.3}% 8
\BOOKMARK [3][-]{subsubsection.1.3.5}{Characteristics of ideal anomaly detector}{subsection.1.3}% 9
\BOOKMARK [3][-]{subsubsection.1.3.6}{General framework of anomaly detectors and summary of each step}{subsection.1.3}% 10
\BOOKMARK [3][-]{subsubsection.1.3.7}{Our Innovation}{subsection.1.3}% 11
\BOOKMARK [3][-]{subsubsection.1.3.8}{Details of sections coming up-next}{subsection.1.3}% 12
\BOOKMARK [1][-]{section.2}{Related Work and State of the art}{}% 13
\BOOKMARK [2][-]{subsection.2.1}{Unsupervised Anomaly Detection}{section.2}% 14
\BOOKMARK [2][-]{subsection.2.2}{Time series Introduction}{section.2}% 15
\BOOKMARK [3][-]{subsubsection.2.2.1}{Time series trend, seasonality and stationary vs non-stationary or concept drift}{subsection.2.2}% 16
\BOOKMARK [2][-]{subsection.2.3}{Time series based deep learning methods}{section.2}% 17
\BOOKMARK [3][-]{subsubsection.2.3.1}{fully connected methods}{subsection.2.3}% 18
\BOOKMARK [3][-]{subsubsection.2.3.2}{CNN based methods}{subsection.2.3}% 19
\BOOKMARK [3][-]{subsubsection.2.3.3}{LSTM based methods}{subsection.2.3}% 20
\BOOKMARK [2][-]{subsection.2.4}{Anomaly detection in time series}{section.2}% 21
\BOOKMARK [2][-]{subsection.2.5}{Unsupervised time series Anomaly detection}{section.2}% 22
\BOOKMARK [1][-]{section.3}{Proposed method}{}% 23
\BOOKMARK [2][-]{subsection.3.1}{General anomaly score based architecture and detailed explanation}{section.3}% 24
\BOOKMARK [2][-]{subsection.3.2}{Autoencoder based architecture}{section.3}% 25
\BOOKMARK [2][-]{subsection.3.3}{Multistep-Ahead prediction based architecture}{section.3}% 26
\BOOKMARK [2][-]{subsection.3.4}{Fully connected multistep ahead prediction architecture}{section.3}% 27
\BOOKMARK [2][-]{subsection.3.5}{Convolutional multistep ahead prediction architecture}{section.3}% 28
\BOOKMARK [2][-]{subsection.3.6}{LSTM multistep ahead prediction architecture}{section.3}% 29
\BOOKMARK [2][-]{subsection.3.7}{Autoencoder architecture}{section.3}% 30
\BOOKMARK [2][-]{subsection.3.8}{Fully Connected Autoencoder architecture}{section.3}% 31
\BOOKMARK [2][-]{subsection.3.9}{Convolutional Autoencoder architecture}{section.3}% 32
\BOOKMARK [2][-]{subsection.3.10}{LSTM Autoencoder architecture}{section.3}% 33
\BOOKMARK [2][-]{subsection.3.11}{Recency concept}{section.3}% 34
\BOOKMARK [2][-]{subsection.3.12}{Post-processing}{section.3}% 35
\BOOKMARK [2][-]{subsection.3.13}{Thresholding}{section.3}% 36
\BOOKMARK [2][-]{subsection.3.14}{Parameterinzing the anomaly score}{section.3}% 37
\BOOKMARK [2][-]{subsection.3.15}{Scoring}{section.3}% 38
\BOOKMARK [1][-]{section.4}{Empirical Formulation and Experiments}{}% 39
\BOOKMARK [2][-]{subsection.4.1}{Dataset}{section.4}% 40
\BOOKMARK [1][-]{section.5}{Results}{}% 41
\BOOKMARK [2][-]{subsection.5.1}{Comparing with others}{section.5}% 42
\BOOKMARK [1][-]{section.6}{Conclusion and Discussion}{}% 43
\BOOKMARK [1][-]{section.7}{Experiment Infrastructure}{}% 44
\BOOKMARK [2][-]{subsection.7.1}{Experiment Management using MLflow}{section.7}% 45
\BOOKMARK [2][-]{subsection.7.2}{Parallel execution using Docker}{section.7}% 46
\BOOKMARK [1][-]{section.8}{Best practices}{}% 47
\BOOKMARK [2][-]{subsection.8.1}{Moving from jupyterlab to pycharm}{section.8}% 48
\BOOKMARK [1][-]{section.9}{Reference Usage}{}% 49
\BOOKMARK [1][-]{section.10}{References}{}% 50
